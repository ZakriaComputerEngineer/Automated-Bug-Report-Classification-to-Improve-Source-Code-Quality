
model:
  name: "microsoft/codebert-base"
  num_labels: 2



training:
  learning: 'Continual'
  progressive_save_dir: "./progressive_save"
  output_dir: "./results"
  results_models: "./results/models"
  results_logs: "./results/logs"
  results_plots: "./results/plots"
  eval_steps: 500
  save_steps: 500
  logging_steps: 500
  load_best_model_at_end: true
  save_total_limit: 3
  evaluation_strategy: "steps"
  save_strategy: "steps"
  logging_dir: "./logs"
  report_to: "tensorboard"
  load_best_model_at_end_metric: "f1"
  greater_is_better: true
  save_strategy: "epoch"
  evaluation_strategy: "epoch"
  save_steps: 1000
  logging_steps: 1000
  eval_accumulation_steps: 1
  eval_delay: 0
  learning_rate: 2e-5
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  num_train_epochs: 50
  weight_decay: 0.01
  label_smoothing_factor: 0.1
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"
  warmup_steps: 500
  metric_for_best_model: "f1"

data:
  max_length: 512
  train_val_split: 0.8
  random_seed: 42
  dataloader_drop_last: false
  dataloader_shuffle: true
  dataloader_batch_size: 16
  dataloader_num_workers: 4
  dataloader_pin_memory: true

testing:
  inference_scheme: "individual"
  trained_model_path: "./results/models/codebert-exp2-model_1"
  individual_save_dir: ",/results/individual_testing"
  results_path: "./results/individual_testing"
  logs_path: "./results/logs"
  
